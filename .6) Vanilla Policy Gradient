import numpy as np
def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)
def policy_gradient(log_probs, rewards, policy_params, learning_rate):
    advantages = rewards
    gradient = np.dot(advantages, log_probs)
    policy_params += learning_rate * gradient
    return policy_params
num_actions, learning_rate = 3, 0.01
policy_params = np.random.rand(num_actions)
rewards = np.array([10, 5, 1])
action_probs = softmax(policy_params)
log_probs = np.log(action_probs)
print("Initial Policy Parameters:", policy_params)
updated_policy_params = policy_gradient(log_probs, rewards, policy_params, learning_rate)
print("Updated Policy Parameters:", updated_policy_params)
print("\nAction Probabilities:")
for action in range(num_actions):
    print(f"  Action {action}: Probability {action_probs[action]:.2f}")
