import numpy as np
reward, gamma, value_next_state, value_current_state, value_current_sarsa, alpha = 10, 0.9, 15, 12,13, 0.1
td_error = reward + gamma * value_next_state - value_current_state
print(f"Temporal Difference (TD) Error: {td_error}")
def update_value(current, td_error, learning_rate):
    return current + learning_rate * td_error
def sarsa_update(current_sarsa, reward, value_next_state, gamma, learning_rate):
    return current_sarsa + learning_rate * (reward + gamma * value_next_state - current_sarsa)
print("\nComparing SARSA and TD updates:")
for step in range(1, 6):
    reward += 2
    value_next_state -= 1
    value_current_state += 0.5
    value_current_sarsa += 0.5
    td_error = reward + gamma * value_next_state - value_current_state
    updated_current_state = update_value(value_current_state, td_error, alpha)
    updated_current_sarsa = sarsa_update(value_current_sarsa, reward, value_next_state, gamma, alpha)
    print(f"\nStep {step}:")
    print(f"  Reward: {reward}")
    print(f"  Temporal Difference (TD) Error: {td_error}")
    print(f"  Updated Value of Current State (TD): {updated_current_state}")
    print(f"  Updated Value of Current State (SARSA): {updated_current_sarsa}")
    value_current_state = updated_current_state
    value_current_sarsa = updated_current_sarsa
