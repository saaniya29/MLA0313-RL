import numpy as np
import random
alpha, gamma, episodes, num_states, num_actions = 0.1, 0.9, 100, 5,2
Q = np.zeros((num_states, num_actions))
def get_reward(state, action):
    return 1 if state == 0 and action == 1 else 0
def choose_action(state, mode='exploration'):
    if mode == 'exploration': return random.choice(range(num_actions))
    return np.argmax(Q[state])
def update_Q_table(state, action, reward, next_state):
    best_next_action = np.argmax(Q[next_state])
    Q[state, action] += alpha * (reward + gamma * best_next_action - Q[state, action])
def train_agent(mode, error=False):
    total_rewards = 0
    for _ in range(episodes):
        state = 0
        while state < num_states - 1:
            action = choose_action(state, mode)
            reward = get_reward(state, action)
            if mode == 'exploitation' and error and state == 0 and action == 0: reward = 0
            reward += random.uniform(0, 0.2) if mode == 'exploration' else 0
            next_state = state + 1
            update_Q_table(state, action, reward, next_state)
            total_rewards += reward
            state = next_state
    return (total_rewards / episodes) * 100
exploration_efficiency = train_agent('exploration')
exploitation_efficiency = train_agent('exploitation', error=True)
print(f'Exploration Efficiency: {exploration_efficiency:.2f}%')
print(f'Exploitation Efficiency: {exploitation_efficiency:.2f}%')
