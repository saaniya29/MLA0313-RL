import numpy as np
reward, gamma, value_next_state, value_current_state, alpha = 10, 0.9, 15, 12, 0.1
td_error = reward + gamma * value_next_state - value_current_state
print(f"Temporal Difference (TD) Error: {td_error}")
def update_value(current, td_error, learning_rate):
    return current + learning_rate * td_error
for step in range(1, 6):
    reward += 2
    value_next_state -= 1
    value_current_state += 0.5
    td_error = reward + gamma * value_next_state - value_current_state
    updated_current_state = update_value(value_current_state, td_error, alpha)
    print(f"\nStep {step}:")
    print(f"  Reward: {reward}")
    print(f"  Temporal Difference (TD) Error: {td_error}")
    print(f"  Updated Value of Current State (TD): {updated_current_state}")
    value_current_state = updated_current_state
